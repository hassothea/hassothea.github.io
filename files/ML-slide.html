<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="keywords" content="remark, remarkjs, markdown, slideshow, presentation" />
    <meta name="description" content="A simple, in-browser, markdown-driven slideshow tool." />
    <title>Supervised ML</title>
    <style>
      /* modified to point to our local separate files */
      @import url("common/fonts.css");
      @import url("common/style.css"); 
    </style>
    <script type="text/javascript"
      src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
    </script>
  </head>

  <body>

<textarea id="source">

class: center, middle

## Supervised machine learning with some .green[green] algorithms

<img src="" width=5%/>

.medium[.name[Sothea Has]]

.small[`sothea.has@lpsm.paris`]

.center[
    <img src="figs/sorbonne.png" style="height: 100px;" />
    <img src="" style="width: 30px;" />
    <img src="figs/lpsm.png" style="height: 150px;" />
    <img src="" style="width: 40px;" />
    <img src="figs/uparis.png" style="height: 100px;" />
]

---

layout: true
class: top

---

class: center, middle, inverse

# &#129300; What for today?

---

# Overview
.pull-left-90[
- .stress[Introduction]
  - What's Machine Learning (ML)?
  - Tranditional Programming (TP) vs Machine Learning (ML)
  - Branches of ML
  - General form of data
- .stress[Supervised Learning]
  - Setting & theory
  - Empirical setting
  - Some inspired methods
- .stress[Tree-based Methods]
  - Decision trees
  - Bagging
  - Random forest
  - Boosting
- .stress[Application]
  - Handwritten digit recognition (`MNIST`)
]

---

class: center, middle, inverse

# ðŸ¤“ Introduction

---

# What is ML?
.pull-left-70[
- An informal defintion by .stress[Arthur Samuel (1959)]:
.center[<br/>"The field of study that gives computers<br/> the ability to learn without being<br/> explicitly programmed."]
]
<br/>
.pull-right-30[
<img src="figs/samuel.PNG" style="height:190px;" />
]

---

# What is ML?
.pull-left-70[
- An informal defintion by .stress[Arthur Samuel (1959)]:
.center[<br/>"The field of study that gives computers<br/> the ability to learn without being<br/> explicitly programmed."]
]
<br/>
.pull-right-30[
<img src="figs/samuel.PNG" style="height:190px;" />
]
.pull-left-80[
- A more formal defintion by .stress[Tom M. Mitchell (1997)]:
]
.pull-right-80[
.center["A computer program is said to learn from experience $E$ with respect to some
  class of tasks $T$ and<br/> performance measure $P$, if its<br/> performance at tasks in $T$,
  as<br/> measured by $P$, improves<br/> with experience $E$"]
]
.pull-right-20[
<img src="figs/mitchell.PNG" style="height:190px;" />
]

---

# TP vs ML

- Tranditional Programming:

<svg width="200" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="130" height="50" 
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Human</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
<br/>
<svg width="200" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Program</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
<br/>
<svg width="215" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="155" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Evaluation</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
<svg width="170" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="100" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="3" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f">&#129060; &nbsp; &nbsp; &nbsp; Input</text>
</svg>
<br/>
<svg width="200" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="120" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Output</text>
</svg>

---

# TP vs ML

- Tranditional Programming:
.pull-right[- Machine Learning]

<svg width="200" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Human</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
.pull-right[<svg width="200" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="70" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Learning</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
<svg width="170" height="100">
  <rect x="35" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="-3" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f">&#129060; &nbsp; Train Data</text>
</svg>
]
<br/>
<svg width="200" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Program</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
.pull-right[<svg width="200" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="130" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Program</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
]
<br/>
<svg width="215" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="155" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Evaluation</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
.pull-right[<svg width="215" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="155" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Evaluation</text>
  <text x="105" y="100" font-family= sans-serif; font-size="45" fill="#ef831f">&#129059;</text>
</svg>
<svg width="170" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="100" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="3" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f">&#129060; &nbsp; &nbsp; &nbsp; Input</text>
</svg>
<svg width="180" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="120" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Output</text>
  </svg>
]
<svg width="170" height="100">
  <rect x="50" y="20" rx="20" ry="20" width="100" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="3" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f">&#129060; &nbsp; &nbsp; &nbsp; Input</text>
</svg>
<br/>
<svg width="200" height="100">
  <text x="110.5" y="19" font-family= sans-serif; font-size="40" fill="#ef831f">&#921;</text>
  <rect x="55" y="20" rx="20" ry="20" width="120" height="50"
  style="fill:white;stroke: #0e87d3;stroke-width:5;opacity:0.8" />
  <text x="80" y="55" font-family= 'Yanone Kaffeesatz'; font-size="35" fill="#ef831f"> Output</text>
</svg>

---
# Branches of ML


<div class="center"><img src="figs/ML_branches.png" style="height:550px;" /></div>

---

# Data
- It can be: images, videos, voices, texts, time series, survey, ...
- Type: numerical, categorical,...
- General form: 
    <table class="container">
      <thead>
        <tr>
          <th>Id</th>
          <th>$X^1$</th>
          <th>$X^2$</th>
          <th>$\dots$</th>
          <th>$X^m$</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>${\bf x}_1$</td>
          <td>$x_1^1$</td>
          <td>$x_1^2$</td>
          <td>$\dots$</td>
          <td>$x_1^m$</td>
        </tr>
        <tr>
          <td>${\bf x}_2$</td>
          <td>$x_2^1$</td>
          <td>$x_2^2$</td>
          <td>$\dots$</td>
          <td>$x_2^m$</td>
        </tr>
        <tr>
          <td>$\dots$</td>
          <td>$\dots$</td>
          <td>$\dots$</td>
          <td>$\dots$</td>
          <td>$\dots$</td>
        </tr>
        <tr>
          <td>${\bf x}_n$</td>
          <td>$x_n^1$</td>
          <td>$x_n^2$</td>
          <td>$\dots$</td>
          <td>$x_n^m$</td>
        </tr>
      </tbody>
    </table>
- Individual: ${\bf x}_i=(x_i^1,...,x_i^m)^T$, for $i=1,...,n$.
- Variable: $X^j=(x_1^j,...,x_n^j)$, for $j=1,...,m$.

---

## First example
- .stress[Iris]: `Hello world` data of .stress1[`ML`] ($150$ rows & $5$ columns).
<table class="container" style="text-align: center; width: 95%;">
  <thead>
    <tr>
      <th>Sepal Lth</th>
      <th>Sepal Wth</th>
      <th>Petal Lth</th>
      <th>Petal Wth</th>
      <th>Species</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$5.1$</td>
      <td>$3.5$</td>
      <td>$1.4$</td>
      <td>$0.2$</td>
      <td>setosa</td>
    </tr>
    <tr>
      <td>$6.9$</td>
      <td>$3.1$</td>
      <td>$4.9$</td>
      <td>$1.5$</td>
      <td>versicolor</td>
    </tr>
    <tr>
      <td>$6.9$</td>
      <td>$3.1$</td>
      <td>$5.4$</td>
      <td>$2.1$</td>
      <td>virginica</td>
    </tr>
    <tr>
      <td>$5.6$</td>
      <td>$3$</td>
      <td>$4.5$</td>
      <td>$1.5$</td>
      <td>versicolor</td>
    </tr>
  </tbody>
</table>
  - Flower ${\bf x_1}=(5.1, 3.5, 1.4, 0.2,\texttt{setosa})$.
  - Variable .small[${\bf Species}=(\texttt{setosa},\texttt{versicolor},\texttt{virginica},\texttt{versicolor})$].
---

## Second example
- .stress[Mnist]: contains totally $70\ 000$ of handwritten digits.
<table class="container" style="text-align: center; width: 95%;">
  <thead>
    <tr>
      <th>Label</th>
      <th>$X_1$</th>
      <th>$X_2$</th>
      <th>$X_3$</th>
      <th>$\dots$</th>
      <th>$X_{784}$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$5$</td>
      <td>$0.0$</td>
      <td>$0.0$</td>
      <td>$0.0$</td>
      <td>$\dots$</td>
      <td>$0.0$</td>
    </tr>
    <tr>
      <td>$\dots$</td>
      <td>$\dots$</td>
      <td>$\dots$</td>
      <td>$\dots$</td>
      <td>$\dots$</td>
      <td>$\dots$</td>
    </tr>
    <tr>
      <td>$8$</td>
      <td>$0.0$</td>
      <td>$0.0$</td>
      <td>$0.0$</td>
      <td>$\dots$</td>
      <td>$0.0$</td>
    </tr>
  </tbody>
</table>

.pull-left-20[<img src="figs/digit_7.png" style="height:200px; width: 380px;" />]
.pull-right[<video width="400px" height="190px" controls autoplay> <source src="figs/digit.mp4" type="video/mp4">
</video>]

---

class: center, middle, inverse

# Supervised Learning
<br/>
## (&#127871;.stress[Movie time]&#127871;)

---

## .stress[Some books you might want to download and never read &#128526;]
.pull-left-50[&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src="figs/distFree.jpg" style="height:150px; width: 100px;" /> 
.pull-right-40[&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src="figs/patternRecg.jpg" style="height:150px; width: 100px;" />]
<br> 
.name[&#128214; <a href="https://link.springer.com/book/10.1007/978-1-4612-0711-5" target="_blank">GyÃ¶rfi et al. (2002)</a> .pull-right-40[.name[&#128214; <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf" target="_blank">Bishop (2006)</a>]]]
<br>
<br>
<br>
<br>
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <img src="figs/ELSL.jpg" style="height:150px; width: 100px;" />
.pull-right-40[&nbsp; &nbsp; &nbsp; &nbsp; <img src="figs/intro-SL.jpg" style="height:150px; width: 150px;" />] 
.pull-right-40[.name[&#128214; <a href="https://www.statlearning.com/" target="_blank">James et al. (2021)</a>]]
<br>
.name[&#128214; <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/https://hastie.su.domains/Papers/ESLII.pdf" target="_blank">Hastie et al. (2008)</a>]  
]

---

# Setting 
## .stress[Notation]
- $(X,Y)\in{\cal X}\times\mathcal{Y}$ : input-output couple & spaces.
- Very often, ${\cal X}=\mathbb{R}^d$, and
  - ${\cal Y}=\mathbb{R}$: .stress1[regression].
  - ${\cal Y}$ is finite ($|{\cal Y}|$ not big): .stress1[classification].

---

# Setting 
## .stress[Notation]
- $(X,Y)\in{\cal X}\times\mathcal{Y}$ : input-output couple & spaces.
- Very often, ${\cal X}=\mathbb{R}^d$, and
  - ${\cal Y}=\mathbb{R}$: .stress1[regression].
  - ${\cal Y}$ is finite ($|{\cal Y}|$ not big): .stress1[classification].

## .stress[Goal]
- Predict $Y$ using $X$ i.e.,
.stress1[.small[$\text{find }f:{\cal X}\to {\cal Y}\text{ s.t }f(X)\approx Y\text{ (in some sense).}$]]

---

# Setting 
## .stress[Notation]
- $(X,Y)\in{\cal X}\times\mathcal{Y}$ : input-output couple & spaces.
- Very often, ${\cal X}=\mathbb{R}^d$, and
  - ${\cal Y}=\mathbb{R}$: .stress1[regression].
  - ${\cal Y}$ is finite ($|{\cal Y}|$ not big): .stress1[classification].

## .stress[Goal]
- Predict $Y$ using $X$ i.e.,
.stress1[.small[$\text{find }f:{\cal X}\to {\cal Y}\text{ s.t }f(X)\approx Y\text{ (in some sense).}$]]

.pull-left[
.hide[Remark]
]
.pull-right-40[<img src="figs/close.png" style="height:200px; width: 300px;" />]

---

# Setting 
## .stress[Notation]
- $(X,Y)\in{\cal X}\times\mathcal{Y}$ : input-output couple & spaces.
- Very often, ${\cal X}=\mathbb{R}^d$, and
  - ${\cal Y}=\mathbb{R}$: .stress1[regression].
  - ${\cal Y}$ is finite ($|{\cal Y}|$ not big): .stress1[classification].

## .stress[Goal]
- Predict $Y$ using $X$ i.e.,
.stress1[.small[$\text{find }f:{\cal X}\to {\cal Y}\text{ s.t }f(X)\approx Y\text{ (in some sense).}$]]

.pull-left[
## .stress[Remark]
- $f$ is linked to .stress1[models].
- .stress1[$f(X)\approx Y$] is linked to .stress1[losses] or .stress1[distances].
]
.pull-right-40[<img src="figs/close.png" style="height:200px; width: 300px;" />]

---

# Setting 
## .stress[Notation]
- $(X,Y)\in{\cal X}\times\mathcal{Y}$ : input-output couple & spaces.
- Very often, ${\cal X}=\mathbb{R}^d$, and
  - ${\cal Y}=\mathbb{R}$: .stress1[regression].
  - ${\cal Y}$ is finite ($|{\cal Y}|$ not big): .stress1[classification].

## .stress[Goal]
- Predict $Y$ using $X$ i.e.,
.stress1[.small[$\text{find }f:{\cal X}\to {\cal Y}\text{ s.t }f(X)\approx Y\text{ (in some sense).}$]]

.pull-left[
## .stress[Remark]
- $f$ is linked to .stress1[models].
- .stress1[$f(X)\approx Y$] is linked to .stress1[losses] or .stress1[distances].
- And many more later, .stress[I promise!]
]
.pull-right-40[<img src="figs/close.png" style="height:200px; width: 300px;" />]

---

# Statistical Decision Theory
## .stress[Assumption]
$(X,Y)$ is assumed to be $\cal X\times Y$-valued .stress1[random variable].
---

# Statistical Decision Theory
## .stress[Assumption]
$(X,Y)$ is assumed to be $\cal X\times Y$-valued .stress1[random variable].
## .stress[Loss functions]
For any $y_1,y_2\in{\cal Y}$, and $p,q\in[0,1]^M$, $\sum_ip_i=\sum_iq_i=1$,
- .stress1[Regression]: 
  - Quadratic or $L_2$ loss: .stress1[$\ell(y_1,y_2)=(y_1-y_2)^2$].
  - Absolute or $L_1$ loss: $\ell(y_1,y_2)=|y_1-y_2|$. 
---

# Statistical Decision Theory 
## .stress[Assumption]
$(X,Y)$ is assumed to be $\cal X\times Y$-valued .stress1[random variable].
## .stress[Loss functions]
For any $y_1,y_2\in{\cal Y}$, and $p,q\in[0,1]^M$, $\sum_ip_i=\sum_iq_i=1$,
- .stress1[Regression]: 
  - Quadratic or $L_2$ loss: .stress1[$\ell(y_1,y_2)=(y_1-y_2)^2$].
  - Absolute or $L_1$ loss: $\ell(y_1,y_2)=|y_1-y_2|$. 
- .stress1[Classification]:
  - 0/1 loss: .stress1[$\ell(y_1, y_2)={\bf 1}(y_1\neq y_2)$].
  - Kullback-leibler divergence: $\ell(p,q)=\sum_{m=1}^Mp_m\log(p_m/q_m)$.
---

# Statistical Decision Theory
## .stress[Assumption]
$(X,Y)$ is assumed to be $\cal X\times Y$-valued .stress1[random variable].
## .stress[Loss functions]
For any $y_1,y_2\in{\cal Y}$, and $p,q\in[0,1]^M$, $\sum_ip_i=\sum_iq_i=1$,
- .stress1[Regression]: 
  - Quadratic or $L_2$ loss: .stress1[$\ell(y_1,y_2)=(y_1-y_2)^2$].
  - Absolute or $L_1$ loss: $\ell(y_1,y_2)=|y_1-y_2|$. 
- .stress1[Classification]:
  - 0/1 loss: .stress1[$\ell(y_1, y_2)={\bf 1}(y_1\neq y_2)$].
  - Kullback-leibler divergence: $\ell(p,q)=\sum_{m=1}^Mp_m\log(p_m/q_m)$.

##.stress[Risk] .pull-right-80[.small[${\cal R(f)}=\mathbb{E}[\ell(f(X),Y)]\ \ \ \ \ (1)$]]
---

# Statistical Decision Theory
## .stress[Goal]
.stress1[$\text{Searching for }f^{\star}\ \text{s.t }{\cal R}(f^{\star})=\text{inf}_{f\in{\cal H}}{\cal R}(f)$]
---

# Statistical Decision Theory
## .stress[Goal]
.stress1[$\text{Searching for }f^{\star}\ \text{s.t }{\cal R}(f^{\star})=\text{inf}_{f\in{\cal H}}{\cal R}(f)$]
## .stress[Some results]
Let $\eta(x)=\mathbb{E}(Y| X = x)$ be the .stress[regression function].
- .stress1[Regression with ${\cal R}(f)=\mathbb{E}[(f(X)-Y)^2]$]

  - ${\cal R}(f)=\mathbb{E}[(f(X)-\eta(X))^2]+\mathbb{E}[(Y-\eta(X))^2]$.
---

# Statistical Decision Theory
## .stress[Goal]
.stress1[$\text{Searching for }f^{\star}\ \text{s.t }{\cal R}(f^{\star})=\text{inf}_{f\in{\cal H}}{\cal R}(f)$]
## .stress[Some results]
Let $\eta(x)=\mathbb{E}(Y| X = x)$ be the .stress[regression function].
- .stress1[Regression with ${\cal R}(f)=\mathbb{E}[(f(X)-Y)^2]$]

  - ${\cal R}(f)=\mathbb{E}[(f(X)-\eta(X))^2]+\mathbb{E}[(Y-\eta(X))^2]$.
  - $f^{\star}=\eta$ (Bayesian estimator).

---
# Statistical Decision Theory
## .stress[Goal]
.stress1[$\text{Searching for }f^{\star}\ \text{s.t }{\cal R}(f^{\star})=\text{inf}_{f\in{\cal H}}{\cal R}(f)$]
## .stress[Some results]
Let $\eta(x)=\mathbb{E}(Y| X = x)$ be the .stress[regression function].
- .stress1[Regression with ${\cal R}(f)=\mathbb{E}[(f(X)-Y)^2]$]

  - ${\cal R}(f)=\mathbb{E}[(f(X)-\eta(X))^2]+\mathbb{E}[(Y-\eta(X))^2]$.
  - $f^{\star}=\eta$ (Bayesian estimator).
  - ${\cal R}(\eta)=\mathbb{E}[(Y-\eta(X))^2]$ (Bayesian risk).

---

# Statistical Decision Theory
## .stress[Goal]
.stress1[$\text{Searching for }f^{\star}\ \text{s.t }{\cal R}(f^{\star})=\text{inf}_{f\in{\cal H}}{\cal R}(f)$]
## .stress[Some results]
Let $\eta(x)=\mathbb{E}(Y| X = x)$ be the .stress[regression function].
- .stress1[Regression with ${\cal R}(f)=\mathbb{E}[(f(X)-Y)^2]$]

  - ${\cal R}(f)=\mathbb{E}[(f(X)-\eta(X))^2]+\mathbb{E}[(Y-\eta(X))^2]$.
  - $f^{\star}=\eta$ (Bayesian estimator).
  - ${\cal R}(\eta)=\mathbb{E}[(Y-\eta(X))^2]$ (Bayesian risk).

- .stress1[$M$-class classification with ${\cal R}(f)=\mathbb{E}[{\bf 1}(f(X)\neq Y)]$]
  - $\displaystyle f^{\star}(x)=\text{arg}\max_{1\leq m\leq M}\mathbb{P}(Y=m|X=x)$.

---

# Empirical Setting (real game!)
## .stress[Setting]
- Observe ${\cal D}_n=\\{({\bf x}_1,y_1), ({\bf x}_2,y_2),..., ({\bf x}_n,y_n)\\}$, where 
$$({\bf x}_i,y_i)\overset{iid}{\sim} (X,Y), \forall i\in\\{1,2,...,n\\}.$$
- The problem is reduced to:
  - Empirical risk: $\displaystyle {\cal R}_n(f)=\frac{1}{n}\sum_1^n\ell(f({\bf x}_i),y_i)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .stress1[$(2)$].
  - Search for $f_n^{\star}$ minimizing ${\cal R}_n(f)$.

## .stress[To make it works!]
- .stress1[Empirical risk minimization]: ${\cal R}_n(f_n^*)\to {\cal R}(f^{\star})$ as $n\to+\infty$?
- Minimizing .stress1[$(2)$] often leads to .stress1[overfitting]!!! 
- We want $f_n^*$ that works well on new observations ($\neq$ distribution?).

---

# Some inspired methods
## .stress[$k$-nearest neighbor ($k$NN)]

- $d$ be a distance (Euclidean).
- $x\in\mathbb{R}^d$, let $(X^x(i),Y^x(i))$ be the couple of $i$th NN of $x$ i.e.,
$$ d(x, X^x(1)) \leq d(x, X^x(2)) \leq ...\leq d(x, X^x(n)).$$
- .stress1[Regression]:
$$\hat{y}=\frac{\sum_{i=1}^kY(i)}{k}$$
- .stress1[Classification]: let $S_m^x=\\{ i:Y^x(i)=m,1\leq i\leq k\\}$,

$$\hat{y}=\text{arg}\max_{1\leq m\leq M}\frac{|S_m^x|}{k}$$

---

# Some inspired methods
## .stress[$k$-nearest neighbor ($k$NN)]

.pull-left-40[.small[<table class="container" style="text-align: center; width: 95%;">
  <thead>
    <tr>
      <th>$X_1$</th>
      <th>$X_2$</th>
      <th>$Y$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$3.3$</td>
      <td>$0.3$</td>
      <td><img src="figs/yellow.png" style="height:20px; width: 20px;" /></td>
    </tr>
    <tr>
      <td>$1.6$</td>
      <td>$2.5$</td>
      <td><img src="figs/purple.png" style="height:20px; width: 20px;" /></td>
    </tr>
    <tr>
      <td>$1.9$</td>
      <td>$0.8$</td>
      <td><img src="figs/yellow.png" style="height:20px; width: 20px;" /></td>
    </tr>
    <tr>
      <td>$0.5$</td>
      <td>$5.1$</td>
      <td><img src="figs/purple.png" style="height:20px; width: 20px;" /></td>
    </tr>
    <tr>
      <td>$...$</td>
      <td>$...$</td>
      <td>$...$</td>
    </tr>
    <tr>
      <td>$2.2$</td>
      <td>$2.8$</td>
      <td><img src="figs/purple.png" style="height:20px; width: 20px;" /></td>
    </tr>
    <tr>
      <td>$2$</td>
      <td>$2$</td>
      <td><img src="figs/red.png" style="height:20px; width: 20px;" /></td>
    </tr>
  </tbody>
</table>]
<br/>
- .stress1[classification]:
  - .stress[$3$NN]: .red[$\bullet$] $=$ .purple[$\bullet$].
  - .stress[$6$NN]: .red[$\bullet$] $=$ .yellow[$\bullet$].
]
.pull-right-60[<img src="figs/knn1.png" style="height:300px; width: 430px;" /> 
- .stress1[Regression]:
  - .stress[$3$NN]:  .red[$\bullet$] $=\frac{2+2.1+1.7}{3}\approx 1.93$.
  - .stress[$6$NN]: .red[$\bullet$] $=\frac{2+2.1+1.7+0.8+1+1.5}{6}\approx 1.52$.]
---

# Some inspired methods
## .stress[Kernel smoothing method]
.pull-right-60[<video width="500px" height="250px" controls autoplay> <source src="figs/kernel.mp4" type="video/mp4">
</video>]
- $K:\mathbb{R}^d\to\mathbb{R}_+$:

$$ \hat{y} =\sum_{i=1}^nW_i(x)Y_i,$$
where, for any $h>0$: $$W_i(x)=\frac{K((x-X_i)/h)}{\sum_j K((x-X_j)/h)}, i=1,2,...,n.$$
- .stress[Remark]: $$\eta(x)=\mathbb{E}(Y|X=x)=\sum_i y_i\mathbb{P}(Y=y_i|X=x)$$.
---

# Key parameter
## .stress[Recall]

- We want $f_n^*$ minimizing some risk ${\cal R}_n$. 
- In general, $f_n^*$ depends on some key parameters $\beta\in\Theta$.
- Example:
      - $k$NN: $k$ is the key.
      - kernel smoothing method: $h$ is the key.
      - linear regression: $Y=\beta_0+\sum_1^m\beta_jX_j$, where $\beta\in\mathbb{R}^{m+1}$ is the key.
      - Neural networks: all weights & neuraons are the key parameters.
      - ...

## .stress[Remark]
- Learning $f_n^*\Leftrightarrow$ learning $\beta\in\Theta$, minimizing the risk ${\cal R}_n$.
- From now, learning a good $f_n^*\Leftrightarrow$ learning a good $\beta$.

---

# Overfitting
- Minimizing .stress1[$(2)$] $\Rightarrow$ too flexible $f_n^{\star}$ (high variance).

.pull-left-20[<img src="figs/overfit2.jpg" style="height:300px; width: 550px;" />]

.pull-right-30[&nbsp;&nbsp;&nbsp;<img src="figs/overfit1.jpg" style="height:200px; width: 200px;"/>
&nbsp;&nbsp;&nbsp;&nbsp; .stress[Bob can't sleep!]]

.pull-left-80[
- Morality: .stress[Don't try to fit to much to the training data!]
- Solution: .stress[Sacrifice accuracy on the training data].
  - Cross-validation
  - Regularization
  - Boostrap ...
]
---

# Model validation technique
## .stress[Data splitting]
- ${\cal D}_n={\cal D}_t\cup {\cal D}_v$: around $75\%$-$25\%$ random partions.
- We look for $f_{\hat{\beta}}$ built on ${\cal D}_t$ s.t: 

$$ \hat{\beta} = \text{arg}\min_{b} {\cal R}_v(f_b), $$

where in this case, ${\cal R}_v$ is computed based on ${\cal D}_v$.
<br/>
<br/>
.pull-right[<img src="figs/train_test.png" style="height:180px; width: 380px;" /> ]
<br>
- .stress[What if we are unlucky?]

---

# Model validation technique
## .stress[$K$-fold cross-validation]
- ${\cal D}_n=\cup_1^K {\cal F}_k$: $K$-fold partition.
- We look for $f_\hat{\beta}$ s.t: 

$$ \hat{\beta} = \text{arg}\min_b \frac{1}{K}\sum_1^K{\cal R}_k(f_b)$$
where for any parameter $b$,
.pull-right[<img src="figs/CV.png" style="height:250px; width: 450px;" /> ].pull-left-50[
`for k = 1, 2, ..., K:`

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $f_b$ `is built using` ${\cal D}_n\setminus{\cal F}_k$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ${\cal R}_k$ `is computed using`  ${\cal F}_k$.
]
---

class: center, middle, inverse

# &#127794; .green[Tree]-based algorithms

---
# Decision .green[trees] &#127797;


---

# Humanized latency numbers

Lets multiply all these durations by a billion

.small[.pure-table.pure-table-striped[
| Memory type                        | Latency      | Human duration                                        |
| :--------------------------------- | -----------: | ----------------------------------------------------: |
| L1 cache reference                 | 0.5 s        | One heart beat (0.5 s)                                |
| L2 cache reference                 | 7 s          | Long yawn                                             | 
| Main memory reference              | 100 s        | Brushing your teeth                                   |
| Send 2K bytes over 1 Gbps network  | 5.5 hr       | From lunch to end of work day                         | 
| SSD random read                    | 1.7 days     | A normal weekend                                      | 
| Read 1 MB sequentially from memory | 2.9 days     | A long weekend                                        |
| Round trip within same datacenter  | 5.8 days     | A medium vacation                                     | 
| Read 1 MB sequentially from SSD    | 11.6 days    | Waiting for almost 2 weeks for a delivery             |
| Disk seek                          | 16.5 weeks   | A semester in university                              |
| Read 1 MB sequentially from disk   | 7.8 months   | Almost producing a new human being                    | 
| Send packet US -> Europe -> US     | 4.8 years    | Average time it takes to complete a bachelor's degree | 
]]

---

## Challenges

Challenges with big datasets

- Large data .stress[don't fit] on a **single** hard-drive
- **One** large machine .stress[can't process or store] **all** the data
- For **computations** how do we .stress[stream data] from the **disk to the different 
layers of memory** ?
- **Concurrent accesses** to the data: disks .stress[cannot] be **read in parallel**

## Solutions

- Combine .stress[several machines] containing **hard drives** and **processors** on a **network**
- Using .stress[commodity hardware]: cheap, common architecture i.e. **processor** + **RAM** + **disk**
- .stress[Scalability] = **more machines** on the network
- .stress[Partition] the data across the machines

<!--
.center[
  <img src="figs/big-data-tease.jpg" style="width: 35%;" />
]
-->

---

## Challenges

Dealing with distributed computations adds **software complexity**

- .stress[Scheduling]: How to **split the work across machines**? Must exploit and optimize data locality since moving data is very expensive
- .stress[Reliability]: How to **deal with failure**? Commodity (cheap) hardware fails more often. @Google [1%, 5%] HD failure/year and 0.2% DIMM failure/year 
- .stress[Uneven performance] of the machines: some nodes are slower than others

## Solutions

- .stress[Schedule], **manage** and **coordinate** threads and resources using appropriate software
- .stress[Locks] to **limit** access to resources
- .stress[Replicate] data for **faster reading** and **reliability**

---

# Is it HPC ?

- **High Performance Computing** (HPC)

- **Parallel computing**

### Comments


- For HPC, scaling-up means using a .stress[bigger machine]

- Huge performance increase for **medium** scale problems

- .stress[Very expensive], specialized machines, lots of processors and memory

### Answer is no !

---

# The Big Data universe

Many technologies combining .stress[software] and .stress[cloud computing]

.center[
  <img src="figs/teasing2.jpg" style="width: 100%;" />
]

---

# The Big Data universe

Often used with/for with .stress[Machine Learning] (or AI)

.center[
  <img src="figs/teasing3.png" style="width: 90%;" />
]

---

# Tools

- Softwares such as .stress[`Spark`] or .stress[`HadoopMR`] (Hadoop Map Reduce) are in charge of these challenges
- They are .stress[distributed compute engines]: softwares that ease the development of distributed algorithms

They run on .stress[clusters] (several machine on a network), managed by a .stress[resource manager] such as :
- **`Yarn` :** 
[https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)
- **`Mesos` :** [http://mesos.apache.org](http://mesos.apache.org)
- **`Kubernetes :` **[https://kubernetes.io](https://kubernetes.io/)

A resource manager ensures that the tasks running on the cluster do not try to use the same resources all at once

---

class: center, middle, inverse

# `Apache Spark`

---

# Apache `Spark`

The course will focus mainly on .stress[`Spark`] for big data processing

.center[
  <img src="figs/spark.png" style="width: 35%;" />

  [https://spark.apache.org](https://spark.apache.org)
]

- `Spark` is an .stress[industrial standard] <br>
(cf [https://spark.apache.org/powered-by.html](https://spark.apache.org/powered-by.html))
- One of the most used .stress[big data processing framework]
- .stress[Open source]

The predecessor of `Spark` is `Hadoop`

---

# `Hadoop`

- `Hadoop` has a simple API and good fault tolerance (tolerance to nodes failing midway through a processing job)

- The cost is lots of .stress[data shuffling] across the network

- With intermediate computations .stress[written to disk] **over the network** which we know is .stress[very time expensive] 

It is made of three components:

- .stress[`HDFS`] (Highly Distributed File System) inspired from `GoogleFileSystem`, see 
.small[[https://ai.google/research/pubs/pub51](https://ai.google/research/pubs/pub51)]

- .stress[`YARN`] (Yet Another Ressource Negociator)

- .stress[`MapReduce`] inspired from Google <br> .small[[https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)]

---

# MapReduce's wordcount example

.center[<img src="figs/WordCountFlow.JPG" width=95%/>]

---

# `Spark`

Advantages of `Spark` over `HadoopMR` ?

- .stress[In-memory storage]: use **RAM** for fast iterative computations
- .stress[Lower overhead] for starting jobs
- .stress[Simple and expressive] with `Scala`, `Python`, `R`, `Java` APIs
- .stress[Higher level libraries] with `SparkSQL`, `SparkStreaming`, etc.

Disadvantages of `Spark` over `HadoopMR` ?
 
- `Spark` requires servers with **more CPU** and **more memory**
- But still much cheaper than HPC

`Spark` is .stress[much faster] than `Hadoop`

- `Hadoop` uses **disk** and **network** 
- `Spark` tries to use **memory** as much as possible for operations while minimizing network use

---

# `Spark` and `Hadoop` comparison

<br>

.pure-table.pure-table-striped[
|                          | HadoopMR     | Spark                           |
| -----------------------: | -----------: | ------------------------------: |
| storage                  | Disk         | in-memory or disk               |
| operations               | Map, reduce  | Map, reduce, join, sample, among many others    |
| execution model          | Batch        | Batch, interactive, streamingÂ   |
| Programming environments | Java         | Scala, Java, Python, R          |
]

---

# `Spark` and `Hadoop` comparison

For **logistic regression** training (a simple **classification** algorithm which requires **several passes** on a dataset)

.center[
  <img src="figs/spark-dev3.png" width=50%/>
]
<br>
.center[
  <img src="figs/logistic-regression.png" width=30%/>
]

---

# The `Spark` stack

.center[<img src="figs/spark_stack.png" width=85%/>]

---


# The `Spark` stack

.center[<img src="figs/spark-env-source.png" width=95%/>]

---

# `Spark` can run "everywhere"

.center[<img src="figs/spark-runs-everywhere.png" width=55%/>]


---

class: center, middle, inverse

# Agenda, tools and references

---

# Tentative agenda for the course

**Weeks 1, 2 and 3** <br> 
The .stress[`Python` data-science stack] for **medium-scale** problems

**Weeks 4 and 5** <br>
Introduction to .stress[`spark`] and its .stress[low-level API]

**Weeks 6, 7 and 8** <br>
`Spark`'s high level API: .stress[`spark.sql`]. Data from different formats and sources

**Week 9** <br>
Run a job on a cluster with .stress[`spark-submit`], monitoring, mistakes and debugging

**Weeks 10, 11, 12** <br>
Introduction to .stress[`spark-streaming`] and a glimpse on other big data technologies

---

# Main tools for the course (tentative...)

### Infrastructure

.center[
<img src="figs/docker.png" width=25%/>
<img src="" width=10%/>
]

### Python stack

.center[
<img src="figs/python.png" width=20%/>
<img src="" width=5%/>
<img src="figs/numpy.jpg" width=18%/>
<img src="" width=5%/>
<img src="figs/pandas.png" width=28%/>
<img src="" width=5%/>
<img src="figs/jupyter_logo.png" width=7%/>
]

### Data Visualization

.center[
<img src="figs/matplotlib.png" width=25%/>
<img src="" width=10%/>
<img src="figs/seaborn.png" width=20%/>
<img src="" width=10%/>
<img src="figs/bokeh.png" width=20%/>
]

---

# Main tools for the course (tentative...)

### Big data processing

.center[
<img src="figs/spark.png" width=20%/>
<img src="" width=10%/>
<img src="figs/pyspark.jpg" width=20%/>
<img src="" width=10%/>
<img src="figs/dask.png" width=10%/>
]

### Data storage / formats / querying

.center[
<img src="figs/sql.jpg" width=20%/>
<img src="" width=5%/>
<img src="figs/orc.png" width=20%/>
<img src="" width=5%/>
<img src="figs/parquet.png" width=30%/>

<img src="figs/json.png" width=20%/>
<img src="" width=15%/>
<img src="figs/hdfs.png" width=25%/>
]

---

# Learning resources

- .stress[Spark Documentation Website]  <br>
.small[[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)]

- .stress[API docs] <br>
.small[[http://spark.apache.org/docs/latest/api/scala/index.html](http://spark.apache.org/docs/latest/api/scala/index.html)] <br>
.small[[http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/)]

- .stress[`Databricks` learning notebooks] <br>
.small[[https://databricks.com/resources](https://databricks.com/resources)]

- .stress[StackOverflow] <br>
.small[[https://stackoverflow.com/tags/apache-spark](https://stackoverflow.com/tags/apache-spark)]  <br>
.small[[https://stackoverflow.com/tags/pyspark](https://stackoverflow.com/tags/pyspark)]

- .stress[More advanced] <br>
.small[[http://books.japila.pl/apache-spark-internals/](http://books.japila.pl/apache-spark-internals/)]

---

# Learning Resources

.pull-left-80[
- .stress[Book]: **"Spark The Definitive Guide"** 
  .small[[http://shop.oreilly.com/product/0636920034957.do](http://shop.oreilly.com/product/0636920034957.do)] <br>
  .tiny[[https://github.com/databricks/Spark-The-Definitive-Guide](https://github.com/databricks/Spark-The-Definitive-Guide)]
]

.pull-right-20[
  <img src="figs/spark_book.gif" style="height: 160px;" />
]

<img src="" style="height: 200px;" />

And the **most important thing is:**

.pull-left[
.stress[.large[Practice!]]
]
.pull-right[
  <img src="figs/wtf.jpg" style="height: 200px;" />
]


---

class: center, middle, inverse

# Data centers

---

# Data centers

Wonder what a .stress[datacenter looks like] ?

- Have a look at [http://www.google.com/about/datacenters](http://www.google.com/about/datacenters)

---

# Data centers

Wonder what a .stress[datacenter looks like] ?

.center[<img src="figs/datacenter2.jpg" width=80%/>]

---

# Data centers

Wonder what a .stress[datacenter looks like] ?

<br>

.center[
<iframe width="672" height="378" src="https://www.youtube.com/embed/avP5d16wEp0" 
        frameborder="0" allowfullscreen>
</iframe>
]

---

class: center, middle, inverse

# Thank you !

</textarea>

  <script src="common/remark-latest.min.js"></script>
  <!-- <script>mermaid.initialize({startOnLoad:true});</script>  -->
    <script>
      var hljs = remark.highlighter.engine;
    </script>
   <script>
      var slideshow = remark.create({
          /* sourceUrl: './slides.md', */
          highlightLanguage: 'remark',
          highlightLines: true 
        });
    </script>
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
	    extensions: ["tex2jax.js", "AMSmath.js", "AMSsymbols.js", "autobold.js"],
	    jax: ["input/TeX", "output/HTML-CSS"],
	    tex2jax: {
	    	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
	      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
	      processEscapes: true
	    },
	    "HTML-CSS": { availableFonts: ["TeX"] }
	  });
	</script>
	<script type="text/javascript"
	    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

  <link rel="stylesheet" href="../_css/pure-min.css">
  <style>
  .smaller-font { font-size:15.65px } 
  </style>
  </body>
</html>

